---
title: "Modeling - xgboost"
output:
  html_document:
    df_print: paged
---

### Load Data

We loaded the pre-processed data saved from the previous step.
The initial dataset contained a total of 15,484 rows, with several variables related to voters' characteristics and political opinions.

```{r load-libs, message=FALSE}
rm(list = ls())
library(dplyr)
library(xgboost)
library(caret)

#load("preprocessed_data.RData")
load("scripts/preprocessed_data_2.RData")
```
### check new feature variable

```{r include=FALSE}
# Check the structure of the main data
colnames(ces_Modeling)

ces_Modeling <- ces_Modeling %>%
  select(-imm_duration, -cps21_rel_imp, -Duration__in_seconds_, -pes21_trust)

add_vars <- c("pes21_paymed", "pes21_senate", "pes21_losetouch", "pes21_hatespeech", 
              "pes21_envirojob", "pes21_govtcare", "pes21_famvalues", "pes21_bilingualism",
              "pes21_equalrights","pes21_fitin","pes21_immigjobs","pes21_foreign", "pes21_emb_satif",
              "pes21_donerm", "pes21_privjobs", "pes21_stdofliving", "pes21_trust", "pes21_newerlife")
feature_cols <- names(ces_Modeling)

duplicated_cols <- intersect(add_vars, feature_cols)
duplicated_cols

length(add_vars)
length(duplicated_cols)
```

```{r include=FALSE}
na_counts <- sapply(ces_Modeling, function(x) sum(is.na(x)))
high_na_cols <- names(na_counts[na_counts > 4000])
na_counts[high_na_cols]


duplicated_cols <- intersect(add_vars, high_na_cols)
duplicated_cols

length(duplicated_cols)
```
```{r}
modeling_var_clean <- ces_Modeling %>%
  select(-all_of(high_na_cols))
#modeling_var_clean <- ces_Modeling[, na_counts <= 9000]

# Remove rows with NA values in any of the features or target
# modeling_var_clean <- modeling_var_clean %>% na.omit()
summary(modeling_var_clean)
```
### Split Data

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(modeling_var_clean)), size = 0.8 * nrow(modeling_var_clean))
train_data <- modeling_var_clean[train_index, ]
test_data <- modeling_var_clean[-train_index, ]
```


### XGBoost - Data Preparation

```{r}
# Convert factor columns to numeric (using your original approach)
train_data_numeric <- train_data %>% mutate_if(is.factor, as.numeric)
test_data_numeric  <- test_data %>% mutate_if(is.factor, as.numeric)

# Define feature columns (exclude 'votechoice')
features <- setdiff(names(train_data_numeric), "votechoice")

# Create feature matrices for train and test
train_matrix <- as.matrix(train_data_numeric[, features])
test_matrix  <- as.matrix(test_data_numeric[, features])

# Create labels
train_label <- as.numeric(train_data_numeric$votechoice) - 1
test_label  <- as.numeric(test_data_numeric$votechoice) - 1

# Compute sample weights
class_counts <- table(train_data_numeric$votechoice)
total_count  <- sum(class_counts)
n_class      <- length(class_counts)

# Class weight = total / (num_classes * class_frequency)
class_weights_map <- total_count / (n_class * class_counts)
sample_weights <- class_weights_map[ as.character(train_data_numeric$votechoice) ]

class_weights_map

# Build XGBoost DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label, weight = sample_weights)
dtest  <- xgb.DMatrix(data = test_matrix,  label = test_label)
```

### XGBoost - Training

```{r}
train_and_evaluate <- function(params, dtrain, dtest, nrounds = 100) {
  # Train model
  model <- xgb.train(
    params  = params,
    data    = dtrain,
    nrounds = nrounds,
    verbose = 0
  )
  
  # Predict probabilities
  pred_prob <- predict(model, newdata = dtest)
  
  # Convert to matrix and get class with max probability
  pred_mat  <- matrix(pred_prob, ncol = n_class, byrow = TRUE)
  pred_idx  <- max.col(pred_mat) - 1
  
  # Convert back to factor labels
  pred_labels <- factor(pred_idx, levels = 0:(n_class-1), labels = levels(train_data$votechoice))
  
  # Generate confusion matrix
  cm <- confusionMatrix(pred_labels, test_data$votechoice)
  
  return(list(model = model, pred = pred_labels, cm = cm))
}
```

```{r}
dtrain_nw <- xgb.DMatrix(data = train_matrix, label = train_label)

params_nw <- list(
  objective        = "multi:softprob",  # Multi-class probability output
  num_class        = n_class,
  eval_metric      = "mlogloss",        # Multi-class log loss
  eta              = 0.1,               # Learning rate
  max_depth        = 6,                 # Max tree depth
  min_child_weight = 1,                 # Min child weight
  subsample        = 0.8,               # Subsample ratio
  colsample_bytree = 0.8,               # Column sample by tree
  gamma            = 0                  # Min loss reduction for split
)
pred_nw <- train_and_evaluate(params_nw, dtrain_nw, dtest, nrounds = 100)
pred_nw$cm
```



```{r}
params_1 <- list(
  objective        = "multi:softprob",  # Multi-class probability output
  num_class        = n_class,
  eval_metric      = "mlogloss",        # Multi-class log loss
  eta              = 0.1,               # Learning rate
  max_depth        = 6,                 # Max tree depth
  min_child_weight = 1,                 # Min child weight
  subsample        = 0.8,               # Subsample ratio
  colsample_bytree = 0.8,               # Column sample by tree
  gamma            = 0                  # Min loss reduction for split
)
pred_1 <- train_and_evaluate(params_1, dtrain, dtest, nrounds = 100)
pred_1$cm
```


```{r}
importance_matrix <- xgb.importance(
  feature_names = colnames(train_matrix),
  model = pred_1$model
)
print(importance_matrix)
xgb.plot.importance(importance_matrix[1:20,])
```

### Adjust XGBoost parameters

```{r}
params_2 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 6,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_2 <- train_and_evaluate(params_2, dtrain, dtest, nrounds = 100)
pred_2$cm

params_3 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_3 <- train_and_evaluate(params_3, dtrain, dtest, nrounds = 100)
pred_3$cm

```

We performed automatic hyperparameter tuning using 5-fold cross-validation.
The process included aggregating the cross-validation results and selecting the best set of parameters based on model performance.

```{r eval=FALSE}
ctrl <- trainControl(
  method = "cv", 
  number = 5, 
  verboseIter = TRUE
)

tune_grid <- expand.grid(
  nrounds = c(100, 200, 500),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 0.8),
  min_child_weight = c(1, 3),
  subsample = c(0.7, 0.8)
)

set.seed(42)
xgb_tuned <- train(
  x = train_matrix,
  y = factor(train_label), 
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = tune_grid,
  metric = "mlogloss"
)

print(xgb_tuned$bestTune)

```

The final output was:

```
Aggregating results
Selecting tuning parameters
Fitting nrounds = 200, max_depth = 4, eta = 0.05, gamma = 0, colsample_bytree = 0.7, min_child_weight = 1, subsample = 0.7 on full training set
```

```{r}
params_4 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_4 <- train_and_evaluate(params_4, dtrain, dtest, nrounds = 200)
pred_4$cm
```

```{r}

params_5 <- params_4
set.seed(42)
cv_result <- xgb.cv(
  params                = params_5,
  data                  = dtrain,
  nrounds               = 500,                
  nfold                 = 5,
  early_stopping_rounds = 50,
  verbose               = 1,
  stratified            = TRUE,               
  showsd                = TRUE,
  metrics               = "mlogloss"
)

best_nrounds <- cv_result$best_iteration
cat("Best nrounds from CV:", best_nrounds, "\n")


pred_5 <- train_and_evaluate(params_5, dtrain, dtest, nrounds = best_nrounds)
pred_5$cm
```



```{r}

num_class <- length(unique(train_label))

# 原来的参数网格
tune_grid <- expand.grid(
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 0.8),
  min_child_weight = c(1, 3),
  subsample = c(0.7, 0.8)
)

results <- list()
iter <- 1

for (i in 1:nrow(tune_grid)) {
  
  params <- list(
    booster = "gbtree",
    objective = "multi:softprob",   # 多分类概率输出
    eval_metric = "mlogloss",       # 多分类 log-loss
    num_class = num_class,
    max_depth = tune_grid$max_depth[i],
    eta = tune_grid$eta[i],
    gamma = tune_grid$gamma[i],
    colsample_bytree = tune_grid$colsample_bytree[i],
    min_child_weight = tune_grid$min_child_weight[i],
    subsample = tune_grid$subsample[i]
  )
  
  set.seed(42)
  cv_model <- xgb.cv(
    params = params,
    data = dtrain,                  # 直接用你之前准备好的 dtrain
    nrounds = 500,                  # 最大迭代轮数
    nfold = 5,
    early_stopping_rounds = 50,
    verbose = 1
  )
  
  best_iter <- cv_model$best_iteration
  best_logloss <- cv_model$evaluation_log$test_mlogloss_mean[best_iter]
  
  results[[iter]] <- cbind(
    tune_grid[i, ],
    best_iteration = best_iter,
    best_logloss = best_logloss
  )
  iter <- iter + 1
}

# 合并并查看最优结果
results_df <- do.call(rbind, results)
results_df <- results_df[order(results_df$best_logloss), ]
print(head(results_df, 10))

```
7:3
max_depth  eta gamma colsample_bytree min_child_weight subsample best_iteration best_logloss
49         4 0.05     0              0.7                3       0.8            164     1.194143
29         4 0.05     1              0.8                3       0.7            164     1.194274
33         4 0.05     0              0.7                1       0.8            164     1.194290
25         4 0.05     0              0.8                3       0.7            164     1.194366
37         4 0.05     1              0.7                1       0.8            164     1.194819
53         4 0.05     1              0.7                3       0.8            166     1.194850
59         4 0.10     0              0.8                3       0.8             76     1.194881
21         4 0.05     1              0.7                3       0.7            164     1.195042
17         4 0.05     0              0.7                3       0.7            164     1.195124
13         4 0.05     1              0.8                1       0.7            164     1.195276

8:2
   max_depth  eta gamma colsample_bytree min_child_weight subsample best_iteration best_logloss
21         4 0.05     1              0.7                3       0.7            185     1.191683
5          4 0.05     1              0.7                1       0.7            186     1.191864
17         4 0.05     0              0.7                3       0.7            185     1.192240
39         4 0.10     1              0.7                1       0.8             91     1.192241
37         4 0.05     1              0.7                1       0.8            180     1.192505
61         4 0.05     1              0.8                3       0.8            175     1.192541
35         4 0.10     0              0.7                1       0.8             91     1.192716
1          4 0.05     0              0.7                1       0.7            185     1.192766
49         4 0.05     0              0.7                3       0.8            168     1.192945
19         4 0.10     0              0.7                3       0.7             88     1.192981

```{r}
params_6 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 3,
  subsample        = 0.8,
  colsample_bytree = 0.7,
  gamma            = 0
)

pred_6 <- train_and_evaluate(params_6, dtrain, dtest, nrounds = 164)
pred_6$cm
```
```{r}
params_7 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 3,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 1
)

pred_7 <- train_and_evaluate(params_7, dtrain, dtest, nrounds = 164)
pred_7$cm
```
