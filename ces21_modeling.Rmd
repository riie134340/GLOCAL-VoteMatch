---
title: "VoteMatch - Modeling"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,         
  autodep = TRUE,       
  cache.path = "cache/" 
)
```

### Load Data

We loaded the pre-processed data saved from the previous step.
The initial dataset contained a total of 15,484 rows, with several variables related to voters' characteristics and political opinions.

```{r load-libs, message=FALSE}
rm(list = ls())
library(dplyr)
library(xgboost)
library(caret)

#load("preprocessed_data.RData")
load("output/preprocessed_data_2.RData")
```

```{r include=FALSE}
# Check the structure of the main data
colnames(ces_Modeling)

ces_Modeling <- ces_Modeling %>%
  select(-imm_duration, -cps21_rel_imp, -Duration__in_seconds_, -pes21_trust)
```

### Handling Missing Values

We first remove columns with a high proportion of missing values (more than 4,000 missing entries out of ~15,000 rows). These columns are unlikely to contribute meaningfully to the model and may even introduce noise.

```{r}
na_counts <- sapply(ces_Modeling, function(x) sum(is.na(x)))
high_na_cols <- names(na_counts[na_counts > 4000])

modeling_var_clean <- ces_Modeling %>%
  select(-all_of(high_na_cols))

summary(modeling_var_clean)
```
Since XGBoost natively supports missing values and handles them automatically during training and prediction. 
It learns the optimal split direction for `NA`s during tree building, so there's no need for imputation or row deletion.

### Data Split

The dataset was split into two groups:

- *train_data*: Training set (80%), used to train the model.
- *test_data*: Testing set (20%), used to evaluate the model's performance.

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(modeling_var_clean)), size = 0.8 * nrow(modeling_var_clean))
train_data <- modeling_var_clean[train_index, ]
test_data <- modeling_var_clean[-train_index, ]
```

### Model Selection

We experimented with several modeling approaches, including **logistic regression** and **random forest**, but found that both struggled to accurately predict smaller political parties such as the Green Party and the People's Party.

After these initial trials, we chose to use **XGBoost**, which provided several advantages:

- Can handle **imbalanced data** well using class weights.
- Supports **missing values** without needing imputation.
- Performs well for **multi-class classification**.

With tuning and weighting, XGBoost gave more balanced results across all parties.

### Data Preparation

```{r}
# Convert factor columns to numeric (using your original approach)
train_data_numeric <- train_data %>% mutate_if(is.factor, as.numeric)
test_data_numeric  <- test_data %>% mutate_if(is.factor, as.numeric)

# Define feature columns (exclude 'votechoice')
features <- setdiff(names(train_data_numeric), "votechoice")

# Create feature matrices for train and test
train_matrix <- as.matrix(train_data_numeric[, features])
test_matrix  <- as.matrix(test_data_numeric[, features])

# Create labels
train_label <- as.numeric(train_data_numeric$votechoice) - 1
test_label  <- as.numeric(test_data_numeric$votechoice) - 1

# Compute sample weights
class_counts <- table(train_data_numeric$votechoice)
total_count  <- sum(class_counts)
n_class      <- length(class_counts)

# Class weight = total / (num_classes * class_frequency)
class_weights_map <- total_count / (n_class * class_counts)
sample_weights <- class_weights_map[ as.character(train_data_numeric$votechoice) ]

# Build XGBoost DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label, weight = sample_weights)
dtest  <- xgb.DMatrix(data = test_matrix,  label = test_label)
```

### XGBoost - Training

We trained the model using the `xgb.train()` function with a fixed number of boosting rounds.
A helper function `train_and_evaluate()` was created to:

- Train the model
- Predict class labels on the test set
- Return a confusion matrix for evaluation

```{r}
train_and_evaluate <- function(params, dtrain, dtest, nrounds) {
  # Train model
  model <- xgb.train(
    params  = params,
    data    = dtrain,
    nrounds = nrounds,
    verbose = 0
  )
  
  # Predict probabilities
  pred_prob <- predict(model, newdata = dtest)
  
  # Convert to matrix and get class with max probability
  pred_mat  <- matrix(pred_prob, ncol = n_class, byrow = TRUE)
  pred_idx  <- max.col(pred_mat) - 1
  
  # Convert back to factor labels
  pred_labels <- factor(pred_idx, levels = 0:(n_class-1), labels = levels(train_data$votechoice))
  
  # Generate confusion matrix
  cm <- confusionMatrix(pred_labels, test_data$votechoice)
  
  return(list(model = model, pred = pred_labels, cm = cm))
}
```

#### Comparing Models With and Without Class Weights

We first tested a model **without using class weights**. While the overall accuracy was higher (63.99%), the model completely failed to predict the Green Party (recall = 0.000) and had very low recall for the People's Party (0.087).

After applying **class weights based on class frequencies**, the overall accuracy dropped to 56.68%, but the model achieved better balance:
- Green Party recall improved to 14.8%
- People's Party recall increased to 56.5%

This confirmed that weighting was necessary to ensure fairer predictions across all parties, especially smaller ones.

```{r}
dtrain_nw <- xgb.DMatrix(data = train_matrix, label = train_label)

params_nw <- list(
  objective        = "multi:softprob",  # Multi-class probability output
  num_class        = n_class,
  eval_metric      = "mlogloss",        # Multi-class log loss
  eta              = 0.1,               # Learning rate
  max_depth        = 6,                 # Max tree depth
  min_child_weight = 1,                 # Min child weight
  subsample        = 0.8,               # Subsample ratio
  colsample_bytree = 0.8,               # Column sample by tree
  gamma            = 0                  # Min loss reduction for split
)
pred_nw <- train_and_evaluate(params_nw, dtrain_nw, dtest, nrounds = 100)
pred_nw$cm
```
```{r}
params_1 <- list(
  objective        = "multi:softprob",  # Multi-class probability output
  num_class        = n_class,
  eval_metric      = "mlogloss",        # Multi-class log loss
  eta              = 0.1,               # Learning rate
  max_depth        = 6,                 # Max tree depth
  min_child_weight = 1,                 # Min child weight
  subsample        = 0.8,               # Subsample ratio
  colsample_bytree = 0.8,               # Column sample by tree
  gamma            = 0                  # Min loss reduction for split
)
pred_1 <- train_and_evaluate(params_1, dtrain, dtest, nrounds = 100)
pred_1$cm
```

### XGBoost Hyperparameters Tuning

To evaluate different hyperparameter tuning strategies, I applied two approaches:

- Automated grid search using caret::train() with 5-fold cross-validation
- Manual tuning using xgb.cv() with log-loss (mlogloss) as the evaluation metric

#### Automatic Tuning with Accuracy

The first approach used caret::train() with a custom parameter grid.
The model was evaluated using 5-fold cross-validation, and the best-performing combination was selected based on validation accuracy.

```{r xgb-tuning, cache=TRUE, message=FALSE}

if (!file.exists("output/xgb_tuned_full_accuracy.rds")) {
  
  ctrl <- trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
    )
  
  tune_grid <- expand.grid(
    nrounds = c(100, 200, 300),
    max_depth = c(4, 6),
    eta = c(0.05, 0.1),
    gamma = c(0, 1),
    colsample_bytree = c(0.7, 0.8),
    min_child_weight = c(1, 3),
    subsample = c(0.7, 0.8)
  )
  
  set.seed(42)
  xgb_tuned <- train(
    x = train_matrix,
    y = factor(train_label), 
    method = "xgbTree",
    trControl = ctrl,
    tuneGrid = tune_grid,
    metric = "accuracy"
  )
  
  saveRDS(xgb_tuned, "output/xgb_tuned_full_accuracy.rds")
}else {
  xgb_tuned <- readRDS("output/xgb_tuned_full_accuracy.rds")
}
```

```{r cache=TRUE}
print(xgb_tuned$bestTune)
```

#### Tuning with Log-loss

The built-in tuning tools (e.g., `caret::train()`) do not fully support multi-class `logloss` (`mlogloss`) as the evaluation metric.  
To work around this, we implemented a **manual grid search** using `xgb.cv()`.

```{r xgb-tuning-log-loss, cache=TRUE}
num_class <- length(unique(train_label))

# Define the grid of hyperparameters to search
tune_grid <- expand.grid(
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 0.8),
  min_child_weight = c(1, 3),
  subsample = c(0.7, 0.8)
)

results <- list()
iter <- 1

for (i in 1:nrow(tune_grid)) {
  # Set parameters for current grid
  params <- list(
    booster = "gbtree",
    objective = "multi:softprob",
    eval_metric = "mlogloss",
    num_class = num_class,
    max_depth = tune_grid$max_depth[i],
    eta = tune_grid$eta[i],
    gamma = tune_grid$gamma[i],
    colsample_bytree = tune_grid$colsample_bytree[i],
    min_child_weight = tune_grid$min_child_weight[i],
    subsample = tune_grid$subsample[i]
  )
  
  set.seed(42)
  cv_model <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 500, # Max training rounds
    nfold = 5,
    early_stopping_rounds = 50,  # Stop early if no improvement in 50 round
    verbose = 1
  )
  
  best_iter <- cv_model$best_iteration
  best_logloss <- cv_model$evaluation_log$test_mlogloss_mean[best_iter]
  
  results[[iter]] <- cbind(
    tune_grid[i, ],
    best_iteration = best_iter,
    best_logloss = best_logloss
  )
  iter <- iter + 1
}

results_df <- do.call(rbind, results)
results_df <- results_df[order(results_df$best_logloss), ]
print(head(results_df, 10))
```

   max_depth  eta gamma colsample_bytree min_child_weight subsample best_iteration best_logloss
21         4 0.05     1              0.7                3       0.7            185     1.191683
5          4 0.05     1              0.7                1       0.7            186     1.191864
17         4 0.05     0              0.7                3       0.7            185     1.192240
39         4 0.10     1              0.7                1       0.8             91     1.192241
37         4 0.05     1              0.7                1       0.8            180     1.192505
61         4 0.05     1              0.8                3       0.8            175     1.192541
35         4 0.10     0              0.7                1       0.8             91     1.192716
1          4 0.05     0              0.7                1       0.7            185     1.192766
49         4 0.05     0              0.7                3       0.8            168     1.192945
19         4 0.10     0              0.7                3       0.7             88     1.192981

```{r}
params_2 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 1,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 0
)
pred_2 <- train_and_evaluate(params_2, dtrain, dtest, nrounds = 200)
pred_2$cm

params_3 <- list(
  objective        = "multi:softprob",
  num_class        = n_class,
  eval_metric      = "mlogloss",
  eta              = 0.05,
  max_depth        = 4,
  min_child_weight = 3,
  subsample        = 0.7,
  colsample_bytree = 0.7,
  gamma            = 1
)
pred_3 <- train_and_evaluate(params_3, dtrain, dtest, nrounds = 200)
pred_3$cm
```
```{r early-stop-1, cache=TRUE}
params_4 <- params_2
set.seed(42)
cv_result <- xgb.cv(
  params                = params_4,
  data                  = dtrain,
  nrounds               = 500,                
  nfold                 = 5,
  early_stopping_rounds = 50,
  verbose               = 0,
  stratified            = TRUE,               
  showsd                = TRUE,
  metrics               = "mlogloss"
)

best_nrounds <- cv_result$best_iteration
cat("Best nrounds from CV:", best_nrounds, "\n")

pred_4 <- train_and_evaluate(params_4, dtrain, dtest, nrounds = best_nrounds)
```

```{r early-stop-2, cache=TRUE}
params_5 <- params_3
set.seed(42)
cv_result <- xgb.cv(
  params                = params_5,
  data                  = dtrain,
  nrounds               = 500,                
  nfold                 = 5,
  early_stopping_rounds = 50,
  verbose               = 0,
  stratified            = TRUE,               
  showsd                = TRUE,
  metrics               = "mlogloss"
)

best_nrounds <- cv_result$best_iteration
cat("Best nrounds from CV:", best_nrounds, "\n")


pred_5 <- train_and_evaluate(params_5, dtrain, dtest, nrounds = best_nrounds)
```
```{r}
pred_4$cm
pred_5$cm
```

### Model Performance Analysis

```{r}
metrics <- c("Sensitivity", "Pos Pred Value", "Balanced Accuracy")

rbind(
  accuracy = c(pred_2$cm$byClass["Class: Green Party", metrics], 
               Accuracy = pred_2$cm$overall["Accuracy"]),
  logloss = c(pred_3$cm$byClass["Class: Green Party", metrics], 
              Accuracy = pred_3$cm$overall["Accuracy"]),
  accuracy_best_nround = c(pred_4$cm$byClass["Class: Green Party", metrics], 
                           Accuracy = pred_4$cm$overall["Accuracy"]),
  logloss_best_nround = c(pred_5$cm$byClass["Class: Green Party", metrics], 
                          Accuracy = pred_5$cm$overall["Accuracy"])
)
```


```{r}
green_sens <- c(
  A = pred_2$cm$byClass["Class: Green Party", "Sensitivity"],
  B = pred_3$cm$byClass["Class: Green Party", "Sensitivity"],
  C = pred_4$cm$byClass["Class: Green Party", "Sensitivity"],
  D = pred_5$cm$byClass["Class: Green Party", "Sensitivity"]
)

barplot(green_sens, main = "Green Party Sensitivity", ylim = c(0, 0.4))

```

```{r}
params_map <- list(
  A = params_2,  # pred_2
  B = params_3,  # pred_3
  C = params_4,  # pred_4
  D = params_5   # pred_5
)

nrounds_map <- c(
  A = 200,
  B = 200,
  C = 185,
  D = 185
)

R <- 20
set.seed(1)
seeds <- sample(1:10000, R)
green_row <- "Class: Green Party"

run_once <- function(params, dtrain, dtest, nrounds, seed) {
  set.seed(seed)
  out <- train_and_evaluate(params, dtrain, dtest, nrounds = nrounds)
  c(
    Accuracy    = as.numeric(out$cm$overall["Accuracy"]),
    Green_Sens  = as.numeric(out$cm$byClass[green_row, "Sensitivity"])
  )
}

res_list <- lapply(names(nrounds_map), function(m){
  mat <- sapply(seeds, function(s) run_once(params_map[[m]], dtrain, dtest, nrounds_map[[m]], s))
  t(mat) %>% as.data.frame() %>% mutate(model = m)
})

res <- dplyr::bind_rows(res_list)

summary_tbl <- res %>%
  dplyr::group_by(model) %>%
  dplyr::summarise(
    Accuracy_mean   = mean(Accuracy),
    Accuracy_sd     = sd(Accuracy),
    GreenSens_mean  = mean(Green_Sens),
    GreenSens_sd    = sd(Green_Sens),
    .groups = "drop"
  ) %>%
  dplyr::arrange(desc(Accuracy_mean))

summary_tbl
```

```{r}
# Scatter plot with error bars (means ± SD)
library(ggplot2)

ggplot(summary_tbl, aes(x = Accuracy_mean, y = GreenSens_mean, color = model)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = GreenSens_mean - GreenSens_sd,
                    ymax = GreenSens_mean + GreenSens_sd),
                width = 0.001) +
  geom_errorbarh(aes(xmin = Accuracy_mean - Accuracy_sd,
                     xmax = Accuracy_mean + Accuracy_sd),
                 height = 0.001) +
  geom_text(aes(label = model), vjust = -1.2, show.legend = FALSE) +
  labs(
    title = "Model Comparison (Mean ± SD)",
    x = "Overall Accuracy (mean ± SD)",
    y = "Green Party Sensitivity (mean ± SD)"
  ) +
  theme_minimal()

```
Across 20 repeated runs, Model A achieved the highest mean overall accuracy (0.553) and the lowest standard deviation (0.00221), indicating both strong performance and high stability.
In terms of Green Party sensitivity, Model C had the highest mean value (0.240), but also showed larger variability (SD = 0.0227), which suggests that its advantage in this metric is less consistent across runs.

Models B and D performed slightly worse overall. Model B’s mean accuracy (0.552) was marginally lower than A’s and it had the lowest mean Green Party sensitivity (0.232). Model D not only had the lowest mean accuracy (0.551) but also the largest variability in both accuracy and sensitivity, making it the least stable choice.


```{r}
# Save model & artifacts
xgb.save(pred_4$model, "model_final.xgb")

final_model <- list(
  model        = pred_4$model,
  features   = setdiff(names(train_data_numeric), "votechoice"),
  class_levels = levels(train_data$votechoice),
  n_class    = length(levels(train_data$votechoice)),
  params     = params_4,
  nrounds    = 185
)

saveRDS(final_model, "output/modeling.rds")
```


```{r include=FALSE}
importance_matrix <- xgb.importance(
  feature_names = colnames(train_matrix),
  model = pred_1$model
)
print(importance_matrix)
xgb.plot.importance(importance_matrix[1:20,])
```